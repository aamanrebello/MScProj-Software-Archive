{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLOU8Za1GNq"
      },
      "source": [
        "# Import Repo of Sepsis Simulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGEp1ymaWUpv",
        "outputId": "19fd58b8-8364-43e3-89ef-0aba32552482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gumbel-max-scm'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (113/113), 1.48 MiB | 4.01 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/clinicalml/gumbel-max-scm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv2brrpP2WlI"
      },
      "outputs": [],
      "source": [
        "#Enable importing code from parent directory\n",
        "import os, sys\n",
        "simulator_path = os.path.abspath('./gumbel-max-scm')\n",
        "sys.path.insert(1, simulator_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpUoCCBH_Ta8",
        "outputId": "f2830268-3c51-4c3d-a6e2-0790ef981904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.10.1)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25657 sha256=bb44fa85cd8dee155a5cd718136824544d1fd5e71045a48790cb4a6c22766117\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymdptoolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VImiXcP9FkB"
      },
      "source": [
        "**IMPORTANT NOTE:** At this stage, to reproduce our experiments, one must modify line 38 of `gumbel-max-scm/sepsisSimDiabetes/DataGenerator.py` so that it reads:\n",
        "\n",
        "```\n",
        "mdp = MDP(init_state_idx=%state%,\n",
        "          policy_array=policy, policy_idx_type=policy_idx_type,\n",
        "          p_diabetes=p_diabetes)\n",
        "\n",
        "```\n",
        "\n",
        "We have essentially set the initial state to a fixed value so that we may estimate the Q-function from that state. Additionally, line 58 of the same file must be modified to:\n",
        "\n",
        "```\n",
        "mdp.state = mdp.get_new_state(state_idx = %state%)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orCP8G0C3DCb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cf.counterfactual as cf\n",
        "import cf.utils as utils\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import itertools as it\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from scipy.linalg import block_diag\n",
        "\n",
        "# Sepsis Simulator code\n",
        "from sepsisSimDiabetes.State import State\n",
        "from sepsisSimDiabetes.Action import Action\n",
        "from sepsisSimDiabetes.DataGenerator import DataGenerator\n",
        "import sepsisSimDiabetes.MDP as simulator\n",
        "\n",
        "import mdptoolboxSrc.mdp as mdptools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "# For optimization\n",
        "from scipy.optimize import Bounds, BFGS\n",
        "from scipy.optimize import LinearConstraint, NonlinearConstraint, minimize\n",
        "# For generating dataset\n",
        "import sklearn.datasets as dt\n",
        "\n",
        "ZERO = 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zt7QLKc3iQ7"
      },
      "source": [
        "# Set up Variables and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH6SMENMD-SV"
      },
      "source": [
        "Code taken from [Oberst and Sontag](https://github.com/clinicalml/gumbel-max-scm/blob/master/plots-main-paper.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gSS-DPD50f"
      },
      "source": [
        "Set up important variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTLB4gBp4Ww-"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "NSIMSAMPS = 100000  # Samples to draw from the simulator\n",
        "NSTEPS = 20  # Max length of each trajectory\n",
        "NCFSAMPS = 5  # Counterfactual Samples per observed sample\n",
        "DISCOUNT_Pol = 0.99 # Used for computing optimal policies\n",
        "DISCOUNT = 1 # Used for computing actual reward\n",
        "PHYS_EPSILON = 0.05 # Used for sampling using physician pol as eps greedy\n",
        "\n",
        "# Option 1: Use bootstrapping w/replacement on the original NSIMSAMPS to estimate errors\n",
        "USE_BOOSTRAP=True\n",
        "N_BOOTSTRAP = 100\n",
        "\n",
        "# Option 2: Use repeated sampling (i.e., NSIMSAMPS fresh simulations each time) to get error bars;\n",
        "# This is done in the appendix of the paper, but not in the main paper\n",
        "N_REPEAT_SAMPLING = 1\n",
        "\n",
        "# These are properties of the simulator, do not change\n",
        "n_actions = Action.NUM_ACTIONS_TOTAL\n",
        "n_components = 2\n",
        "\n",
        "# These are added as absorbing states\n",
        "n_states_abs = State.NUM_OBS_STATES + 2\n",
        "discStateIdx = n_states_abs - 1\n",
        "deadStateIdx = n_states_abs - 2\n",
        "\n",
        "# Number of runs for calculating MSE\n",
        "RUNS = 20\n",
        "# Number of episodes over which we average an OPE estimate\n",
        "N = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUORk1KEedLa"
      },
      "outputs": [],
      "source": [
        "hr_state_mapping = ['Low', 'Normal', 'High']\n",
        "sbp_state_mapping = ['Low', 'Normal', 'High']\n",
        "o2_state_mapping = ['Low', 'Normal']\n",
        "glu_state_mapping = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
        "abx_state_mapping = ['Off', 'On']\n",
        "vaso_state_mapping = ['Off', 'On']\n",
        "vent_state_mapping = ['Off', 'On']\n",
        "diab_state_mapping = ['No', 'Yes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2A-p4iXLLVn"
      },
      "source": [
        "Set up base for behaviour and evaluation policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmXqCQquBR-b"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"gumbel-max-scm/data/diab_txr_mats-replication.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"gumbel-max-scm/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHSM6i7nAJwg"
      },
      "outputs": [],
      "source": [
        "# Get the transition and reward matrix from file\n",
        "with open(\"gumbel-max-scm/data/diab_txr_mats-replication.pkl\", \"rb\") as f:\n",
        "    mdict = pickle.load(f)\n",
        "\n",
        "tx_mat = mdict[\"tx_mat\"]\n",
        "r_mat = mdict[\"r_mat\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL5_-lEZBxh6"
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import block_diag\n",
        "\n",
        "tx_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
        "r_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
        "\n",
        "# Easily accessible variables\n",
        "A = n_actions\n",
        "S = State.NUM_FULL_STATES\n",
        "\n",
        "for a in range(n_actions):\n",
        "    tx_mat_full[a, ...] = block_diag(tx_mat[0, a, ...], tx_mat[1, a,...])\n",
        "    r_mat_full[a, ...] = block_diag(r_mat[0, a, ...], r_mat[1, a, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc3sZU3pB-xp"
      },
      "outputs": [],
      "source": [
        "fullMDP = cf.MatrixMDP(tx_mat_full, r_mat_full)\n",
        "fullPol = fullMDP.policyIteration(discount=DISCOUNT_Pol, eval_type=1)\n",
        "\n",
        "#The behavior policy is the fully random policy\n",
        "randPol = np.ones(fullPol.shape)/(fullPol.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqM8gH597gjk"
      },
      "outputs": [],
      "source": [
        "#We want the expected reward of starting in a state and taking an action\n",
        "R = np.swapaxes(np.mean(r_mat_full, axis=-1), 0, 1)\n",
        "R.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOSZ8n6_R24G"
      },
      "outputs": [],
      "source": [
        "#To handle -1 states and -1 actions\n",
        "def pad_policy(policy, val=1):\n",
        "  #Add a column of zeroes to the end\n",
        "  policy = np.concatenate((policy, np.full((policy.shape[0], 1), val)), axis=1)\n",
        "  #Add a row of zeroes at the end\n",
        "  policy = np.concatenate((policy, np.full((1, policy.shape[1]), val)), axis=0)\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W3Fj_q5EADA"
      },
      "source": [
        "# Load repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY1b2UIxA9Ds"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ai4ai-lab/Factored-Action-Spaces-for-OPE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4etBTF2ubmdb"
      },
      "outputs": [],
      "source": [
        "#Enable importing code from parent directory\n",
        "import os, sys\n",
        "main_folder = os.path.abspath('./Factored-Action-Spaces-for-OPE')\n",
        "sys.path.insert(1, main_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SYkHKlIMRj8"
      },
      "source": [
        "# From Patient State 136, With Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuOvmq-cMxFP"
      },
      "outputs": [],
      "source": [
        "#The patient has diabetes\n",
        "PROB_DIAB = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFB9lbC6M2Ks"
      },
      "source": [
        "### State Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrzAryBORmvz"
      },
      "outputs": [],
      "source": [
        "#Instantiate a state based on the idx and get the state vector\n",
        "testState = State(state_idx = 136, diabetic_idx=1)\n",
        "vec = testState.get_state_vector()\n",
        "\n",
        "print(vec)\n",
        "\n",
        "print(f'Heart Rate: {hr_state_mapping[vec[0]]}')\n",
        "print(f'Systolic Blood Pressure: {sbp_state_mapping[vec[1]]}')\n",
        "print(f'Percent Oxygen: {o2_state_mapping[vec[2]]}')\n",
        "print(f'Glucose Level: {glu_state_mapping[vec[3]]}')\n",
        "print(f'Antibiotics: {abx_state_mapping[vec[4]]}')\n",
        "print(f'Vasopressors: {vaso_state_mapping[vec[5]]}')\n",
        "print(f'Ventilator: {vent_state_mapping[vec[6]]}')\n",
        "print(f'Diabetes: {testState.diabetic_idx}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPF8wN8gNZEK"
      },
      "source": [
        "### Generate Data From Behaviour Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGbGvpAiLOr0"
      },
      "source": [
        "Run the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSAmbkYoDQRe"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=randPol, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bhLMTrLQ__"
      },
      "source": [
        "Convert data into array format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsk5poscGNBH",
        "outputId": "76b7382c-37a5-42ee-e557-4e58485af9ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_b = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSY3QNm8wgKb",
        "outputId": "83f4e212-17a5-46b9-d090-f77afc8482e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[  0. 136.   0.  -1. 168.]\n",
            "  [  1. 168.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   3.  -1. 227.]\n",
            "  [  1. 227.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.   0. 223.]\n",
            "  [  1. 223.   3.   0. 219.]\n",
            "  [  2. 219.   1.   0. 218.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0. 136.   2.   0. 377.]\n",
            "  [  1. 377.   4.   0. 372.]\n",
            "  [  2. 372.   7.   0. 463.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   5.   0. 222.]\n",
            "  [  1. 222.   1.   0. 218.]\n",
            "  [  2. 218.   7.  -1. 231.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.  -1. 231.]\n",
            "  [  1. 231.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]]\n"
          ]
        }
      ],
      "source": [
        "print(nf_tr_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcRSeGkJITQZ"
      },
      "outputs": [],
      "source": [
        "randPol = pad_policy(randPol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljqSoSVUI_7u"
      },
      "source": [
        "### Varying Episodes $\\epsilon_{e} = 0.4$ (Policy Divergence $4.8^{20}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WGmgmigI_7v"
      },
      "source": [
        "Set up evaluation policy, generate data and convert into factored format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Kvf7JwI_7v"
      },
      "outputs": [],
      "source": [
        "EVAL_EPSILON = 0.4\n",
        "\n",
        "evalPolSoft = np.copy(fullPol)\n",
        "evalPolSoft[evalPolSoft == 1] = 1 - EVAL_EPSILON\n",
        "evalPolSoft[evalPolSoft == 0] = EVAL_EPSILON / (n_actions - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT2KzVpCI_7v",
        "outputId": "7518988c-9fe9-44d1-e9c1-18084b2fa472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.8\n"
          ]
        }
      ],
      "source": [
        "# Calculate policy divergence from Voloshin et al.\n",
        "D = 0\n",
        "for state in range(randPol.shape[0] - 1):\n",
        "    for action in range(randPol.shape[1] - 1):\n",
        "        difference = evalPolSoft[state, action]/randPol[state, action]\n",
        "        D = max(D, difference)\n",
        "print(D)\n",
        "shorter_D = round(D,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TreqvybTI_7w"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=evalPolSoft, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69P47oFMI_7w",
        "outputId": "7f8ce9a0-d5e7-4dd7-d7dc-aebc31fb323c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_e = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHiTPTj1VYX",
        "outputId": "7016b9fd-8eb2-472a-e2fb-94cdc63e8906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[  0. 136.   0.   0. 136.]\n",
            "  [  1. 136.   2.   0.  57.]\n",
            "  [  2.  57.   3.  -1. 227.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0.  57.]\n",
            "  [  1.  57.   4.  -1.  68.]\n",
            "  [  2.  68.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0. 449.]\n",
            "  [  1. 449.   5.   0. 462.]\n",
            "  [  2. 462.   7.   0. 471.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0. 136.   2.   0. 145.]\n",
            "  [  1. 145.   3.   0. 147.]\n",
            "  [  2. 147.   2.   0.  57.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.   0. 223.]\n",
            "  [  1. 223.   6.   0. 221.]\n",
            "  [  2. 221.   4.   0. 132.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0.  57.]\n",
            "  [  1.  57.   3.   0. 147.]\n",
            "  [  2. 147.   2.   0. 145.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]]\n"
          ]
        }
      ],
      "source": [
        "print(nf_tr_e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GDxxjn6aDbP"
      },
      "outputs": [],
      "source": [
        "evalPolSoft = pad_policy(evalPolSoft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8XjkwrawIRf"
      },
      "outputs": [],
      "source": [
        "unique_states = np.unique(nf_tr_b[:, :, 1])\n",
        "unique_states = unique_states[unique_states != -1].astype(int)\n",
        "Sunq = len(unique_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQkYpbHZw1C0"
      },
      "outputs": [],
      "source": [
        "unique_actions = np.unique(nf_tr_b[:, :, 2])\n",
        "unique_actions = unique_actions[unique_actions != -1].astype(int)\n",
        "Aunq = len(unique_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHUYZOLf_o_R"
      },
      "outputs": [],
      "source": [
        "import policy_estimators as pe\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import gc\n",
        "\n",
        "# Objective function\n",
        "#alpha has shape (SxAXDx3)\n",
        "def OPE_MSE(alpha, discount_factor, D):\n",
        "    boundary = D*Aunq*Sunq\n",
        "    pi_b_d = alpha[0:boundary]\n",
        "    pi_e_d = alpha[boundary:2*boundary]\n",
        "    r_d = alpha[2*boundary:]\n",
        "    print(pi_b_d)\n",
        "    print(pi_e_d)\n",
        "    print(r_d)\n",
        "    #Reshape policies and reward\n",
        "    factored_pi_b = np.reshape(pi_b_d, newshape=(D, Sunq, Aunq))\n",
        "    factored_pi_e = np.reshape(pi_e_d, newshape=(D, Sunq, Aunq))\n",
        "    factored_r = np.reshape(r_d, newshape=(D,Sunq,Aunq))\n",
        "    #Expand actions to full\n",
        "    expanded1_pi_b = np.zeros((D,Sunq,A))\n",
        "    expanded1_pi_e = np.zeros((D,Sunq,A))\n",
        "    expanded1_r = np.zeros((D,Sunq,A))\n",
        "    expanded1_pi_b[:, :, unique_actions] = factored_pi_b\n",
        "    expanded1_pi_e[:, :, unique_actions] = factored_pi_e\n",
        "    expanded1_r[:, :, unique_actions] = factored_r\n",
        "    #Expand states to full\n",
        "    expanded2_pi_b = np.zeros((D,S,A))\n",
        "    expanded2_pi_e = np.zeros((D,S,A))\n",
        "    factored_r = np.zeros((D,S,A))\n",
        "    expanded2_pi_b[:, unique_states, :] = expanded1_pi_b\n",
        "    expanded2_pi_e[:, unique_states, :] = expanded1_pi_e\n",
        "    factored_r[:, unique_states, :] = expanded1_r\n",
        "\n",
        "    #Pad each factored policy\n",
        "    action_spaces = [i for i in range(D)]\n",
        "    factored_pi_b = []\n",
        "    factored_pi_e = []\n",
        "    for d in action_spaces:\n",
        "      factored_pi_b.append(pad_policy(expanded2_pi_b[d, :, :]))\n",
        "      factored_pi_e.append(pad_policy(expanded2_pi_e[d, :, :]))\n",
        "    gc.collect()\n",
        "\n",
        "    estimates = np.zeros(RUNS)\n",
        "\n",
        "    #Obtain OPE estimates from each run\n",
        "    for run in range(RUNS):\n",
        "      transition_data = nf_tr_b[run*N:(run+1)*N, :, :].astype(int)\n",
        "      factored_transition_data = np.zeros((N, NSTEPS, D, 5))\n",
        "      #Generate factored transition data based on transition data and variables\n",
        "      global rhos\n",
        "      for d in range(D):\n",
        "        factored_transition_data[:, :, d, :] = transition_data[:, :, :]\n",
        "        factored_transition_data[:, :, d, 3] = factored_r[d, transition_data[:, :, 1], transition_data[:, :, 2]]\n",
        "\n",
        "        #Transition probabilities\n",
        "        s_list = factored_transition_data[:,:,d,1].astype(int)\n",
        "        a_list = factored_transition_data[:,:,d,2].astype(int)\n",
        "        p_b = factored_pi_b[d][s_list, a_list]\n",
        "        p_e = factored_pi_e[d][s_list, a_list]\n",
        "\n",
        "        # Per-trajectory cumulative importance ratios, take the product\n",
        "        rhos[run, d] = np.mean((p_e / p_b).prod(-1))\n",
        "\n",
        "      #Obtain OPE estimate\n",
        "      OPE_estimate, _ = pe.off_policy_DecIS_estimator(factored_transition_data,\n",
        "                                                discount_factor,\n",
        "                                                action_spaces,\n",
        "                                                factored_pi_b,\n",
        "                                                factored_pi_e)\n",
        "      estimates[run] = OPE_estimate\n",
        "      gc.collect()\n",
        "\n",
        "    #Obtain on policy estimate\n",
        "    on_policy_estimate = pe.on_policy_Q_estimate(nf_tr_e, discount_factor)\n",
        "    #Calculate MSE\n",
        "    MSE = mean_squared_error(estimates, [on_policy_estimate]*RUNS)\n",
        "\n",
        "    global avg_estimate, on_policy_val\n",
        "    avg_estimate = np.mean(estimates)\n",
        "    on_policy_val = on_policy_estimate\n",
        "\n",
        "    print(MSE)\n",
        "    print('--------------------')\n",
        "    return MSE/10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-yDo4h52LGx"
      },
      "outputs": [],
      "source": [
        "def jac(alpha, discount_factor, D):\n",
        "\n",
        "  boundary = D*Aunq*Sunq\n",
        "  gradvals = np.zeros(3*boundary)\n",
        "\n",
        "  #Calculate bias\n",
        "  global avg_estimate, on_policy_val\n",
        "  bias = avg_estimate - on_policy_val\n",
        "\n",
        "  gradvals[:boundary] = 2*bias*avg_estimate/N\n",
        "  gradvals[:boundary] = gradvals[:boundary]/alpha[:boundary]\n",
        "\n",
        "  gradvals[boundary:2*boundary] = -2*bias*avg_estimate/N\n",
        "  gradvals[boundary:2*boundary] = gradvals[boundary:2*boundary]/alpha[boundary:2*boundary]\n",
        "  global rhos\n",
        "  avg_rhos = np.mean(rhos, axis=0)\n",
        "  mini = Aunq*Sunq\n",
        "  for d in range(D):\n",
        "    gradvals[2*boundary + d*mini: 2*boundary + (d+1)*mini] = (2*bias*discount_factor/N)*avg_rhos[d]\n",
        "  gc.collect()\n",
        "  print(gradvals)\n",
        "  print(gradvals.shape)\n",
        "  return gradvals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBVZ4QB72g3r"
      },
      "outputs": [],
      "source": [
        "def hess(alpha, discount_factor, D):\n",
        "  boundary = D*Aunq*Sunq\n",
        "  pi_b_d = alpha[0:boundary]\n",
        "  pi_e_d = alpha[boundary:2*boundary]\n",
        "  r_d = alpha[2*boundary:]\n",
        "  print(pi_b_d)\n",
        "  print(pi_e_d)\n",
        "  print(r_d)\n",
        "  #Reshape policies and reward\n",
        "  factored_pi_b = np.reshape(pi_b_d, newshape=(D, Sunq, Aunq))\n",
        "  factored_pi_e = np.reshape(pi_e_d, newshape=(D, Sunq, Aunq))\n",
        "  factored_r = np.reshape(r_d, newshape=(D,Sunq,Aunq))\n",
        "  #Expand actions to full\n",
        "  expanded1_pi_b = np.zeros((D,Sunq,A))\n",
        "  expanded1_pi_e = np.zeros((D,Sunq,A))\n",
        "  expanded1_r = np.zeros((D,Sunq,A))\n",
        "  expanded1_pi_b[:, :, unique_actions] = factored_pi_b\n",
        "  expanded1_pi_e[:, :, unique_actions] = factored_pi_e\n",
        "  expanded1_r[:, :, unique_actions] = factored_r\n",
        "  #Expand states to full\n",
        "  expanded2_pi_b = np.zeros((D,S,A))\n",
        "  expanded2_pi_e = np.zeros((D,S,A))\n",
        "  factored_r = np.zeros((D,S,A))\n",
        "  expanded2_pi_b[:, unique_states, :] = expanded1_pi_b\n",
        "  expanded2_pi_e[:, unique_states, :] = expanded1_pi_e\n",
        "  factored_r[:, unique_states, :] = expanded1_r\n",
        "\n",
        "  #Pad each factored policy\n",
        "  action_spaces = [i for i in range(D)]\n",
        "  factored_pi_b = []\n",
        "  factored_pi_e = []\n",
        "  for d in action_spaces:\n",
        "    factored_pi_b.append(pad_policy(expanded2_pi_b[d, :, :]))\n",
        "    factored_pi_e.append(pad_policy(expanded2_pi_e[d, :, :]))\n",
        "  gc.collect()\n",
        "\n",
        "  estimates = np.zeros(RUNS)\n",
        "\n",
        "  #Obtain OPE estimates from each run\n",
        "  for run in range(RUNS):\n",
        "    transition_data = nf_tr_b[run*N:(run+1)*N, :, :].astype(int)\n",
        "    decomposed_Qs = np.zeros(len(action_spaces))\n",
        "    for d in range(D):\n",
        "      t_list = transition_data[:, :, 0].astype(int)\n",
        "      s_list = transition_data[:, :, 1].astype(int)\n",
        "      a_list = transition_data[:, :, 2].astype(int)\n",
        "      r_list = factored_r[d, s_list, a_list]\n",
        "\n",
        "      # Per-trajectory returns (discounted cumulative rewards)\n",
        "      G = (r_list * np.power(discount_factor, t_list)).sum(axis=-1)\n",
        "\n",
        "      #Transition probabilities\n",
        "      p_b = factored_pi_b[d][s_list, a_list]\n",
        "      p_e = factored_pi_e[d][s_list, a_list]\n",
        "\n",
        "      # Per-trajectory cumulative importance ratios, take the product\n",
        "      rho = (p_e / p_b).prod(-1)\n",
        "      prod = G*rho[-1]\n",
        "      #Weight with IS ratio then average\n",
        "      decomposed_Qs[d] = np.average(prod)\n",
        "      gc.collect()\n",
        "    #Sum up decomposed Q estimates\n",
        "    estimates[run] = np.sum(decomposed_Qs)\n",
        "\n",
        "  #Obtain on policy estimate\n",
        "  on_policy_estimate = pe.on_policy_Q_estimate(nf_tr_e, discount_factor)\n",
        "  #Calculate MSE\n",
        "  bias = np.mean(estimates) - on_policy_estimate\n",
        "\n",
        "  gradvals = np.full(3*boundary, 2*bias)\n",
        "  gradvals[boundary:2*boundary] = -2*bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIsmc6w8AbSo"
      },
      "outputs": [],
      "source": [
        "def optimize_alpha(discount_factor, D, max_R, factor_pi_b=False, factor_pi_e=False):\n",
        "    np.random.seed(1)\n",
        "    boundary = D*Aunq*Sunq\n",
        "    # Initialize alphas to random values\n",
        "    alpha_0 = np.zeros(boundary*3)\n",
        "    alpha_0[:boundary//2] = (randPol[unique_states, :])[:, unique_actions].flatten()\n",
        "    alpha_0[boundary//2:boundary] = (randPol[unique_states, :])[:, unique_actions].flatten()\n",
        "    alpha_0[boundary:3*boundary//2] = (evalPolSoft[unique_states, :])[:, unique_actions].flatten()\n",
        "    alpha_0[3*boundary//2:2*boundary] = (evalPolSoft[unique_states, :])[:, unique_actions].flatten()\n",
        "    alpha_0[2*boundary:5*boundary//2] = (R[unique_states, :])[:, unique_actions].flatten()/D\n",
        "    alpha_0[5*boundary//2:] = (R[unique_states, :])[:, unique_actions].flatten()/D\n",
        "    #Set bounds on policy and reward\n",
        "    lower_bound = np.zeros(boundary*3)\n",
        "    lower_bound[:2*boundary] = alpha_0[:2*boundary]\n",
        "    lower_bound[2*boundary:] = -max_R\n",
        "    upper_bound = np.full(boundary*3, 1)\n",
        "    upper_bound[2*boundary:] = max_R\n",
        "    # Define the bounds\n",
        "    bounds_alpha = Bounds(lower_bound, upper_bound)\n",
        "    gc.collect()\n",
        "    print(f'Checkpoint 1 {3*boundary}')\n",
        "    #Set linear and non-linear constraints on policy and reward\n",
        "    constraints = []\n",
        "    #Policies must add up to 1\n",
        "    constraint_base = np.zeros(D*Sunq*Aunq*3)\n",
        "    for d in range(D):\n",
        "      for s in range(Sunq):\n",
        "        start_index = d*Sunq*Aunq + s*Aunq\n",
        "        constraint_base[:] = 0.0\n",
        "        constraint_base[start_index:start_index + Aunq] = 1\n",
        "        constraints.append(LinearConstraint(constraint_base, [1], [1]))\n",
        "        gc.collect()\n",
        "        constraint_base[:] = 0.0\n",
        "        constraint_base[boundary + start_index:boundary + start_index + Aunq] = 1\n",
        "        constraints.append(LinearConstraint(constraint_base, [1], [1]))\n",
        "        gc.collect()\n",
        "    print('Checkpoint 2')\n",
        "    #Factored rewards must add to full reward\n",
        "    for s in range(Sunq):\n",
        "      for a in range(Aunq):\n",
        "        constraint_base[:] = 0.0\n",
        "        index_list = np.arange(D)*Sunq*Aunq + (s*Aunq + a + 2*boundary)\n",
        "        constraint_base[index_list] = 1\n",
        "        constraints.append(LinearConstraint(constraint_base, [R[s,a]], [R[s,a]]))\n",
        "        gc.collect()\n",
        "        #If we know that the behaviour policy can be factored\n",
        "        if factor_pi_b:\n",
        "          constraint_base[:] = 0.0\n",
        "          index_list = np.arange(D)*Sunq*Aunq + (s*Aunq + a)\n",
        "          constraint_base[index_list] = 1\n",
        "          def prodf(x):\n",
        "            masked_arr = x*constraint_base\n",
        "            return np.prod(masked_arr, where=(masked_arr!=0))\n",
        "          constraints.append(NonlinearConstraint(prodf, [randPol[s,a]], [randPol[s,a]]))\n",
        "        #If we know that the evaluation policy can be factored\n",
        "        if factor_pi_e:\n",
        "          constraint_base[:] = 0.0\n",
        "          index_list = np.arange(D)*Sunq*Aunq + (s*Aunq + a + 1*boundary)\n",
        "          constraint_base[index_list] = 1\n",
        "          def prodf(x):\n",
        "            masked_arr = x*constraint_base\n",
        "            return np.prod(masked_arr, where=(masked_arr!=0))\n",
        "          constraints.append(NonlinearConstraint(prodf, [evalPolSoft[s,a]], [evalPolSoft[s,a]]))\n",
        "\n",
        "    print('Checkpoint 3')\n",
        "    # Find the optimal value of alpha\n",
        "    result = minimize(OPE_MSE, alpha_0, args = (discount_factor, D), method='SLSQP', jac=jac,\n",
        "                      #hess=BFGS(),\n",
        "                      constraints=constraints,\n",
        "                      bounds=bounds_alpha)\n",
        "    # The optimized value of alpha lies in result.x\n",
        "    alpha = result.x\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0dpRtDzGcIN",
        "outputId": "6ee67875-0d87-4785-9a07-d13552bcf8e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint 1 24048\n",
            "Checkpoint 2\n",
            "Checkpoint 3\n",
            "[0.125 0.125 0.125 ... 0.125 0.125 0.125]\n",
            "[0.05714286 0.05714286 0.05714286 ... 0.05714286 0.6        0.05714286]\n",
            "[-0.14409722 -0.14409722 -0.14409722 ... -0.14409722 -0.14409722\n",
            " -0.14409722]\n",
            "3.5084325843666107\n",
            "--------------------\n",
            "[ 0.01048075  0.01048075  0.01048075 ... -0.00100111 -0.00100111\n",
            " -0.00100111]\n",
            "(24048,)\n"
          ]
        }
      ],
      "source": [
        "max_R = np.max(r_mat_full)\n",
        "D = 2\n",
        "#Define global variables\n",
        "rhos = np.zeros((RUNS, D))\n",
        "avg_estimate = 0\n",
        "on_policy_val = 0\n",
        "factorisation = optimize_alpha(DISCOUNT_Pol, D, max_R, factor_pi_b=False, factor_pi_e=False)\n",
        "boundary = D*Sunq*Aunq\n",
        "print(factorisation[0:boundary])\n",
        "print(factorisation[boundary:2*boundary])\n",
        "print(factorisation[2*boundary:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cX3JwX7HVpx"
      },
      "outputs": [],
      "source": [
        "max_R = np.max(r_mat_full)\n",
        "D = 3\n",
        "factorisation = optimize_alpha(DISCOUNT_Pol, D, max_R, factor_pi_b=False, factor_pi_e=False)\n",
        "boundary = D*S*A\n",
        "print(factorisation[0:boundary])\n",
        "print(factorisation[boundary:2*boundary])\n",
        "print(factorisation[2*boundary:])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7Zt7QLKc3iQ7",
        "5W3Fj_q5EADA",
        "TPF8wN8gNZEK"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
