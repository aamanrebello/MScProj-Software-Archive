{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgLOU8Za1GNq"
      },
      "source": [
        "# Import Repo of Sepsis Simulator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGEp1ymaWUpv",
        "outputId": "4559d2ce-0fec-4e55-8819-6292277d4a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gumbel-max-scm'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 113 (delta 0), reused 0 (delta 0), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (113/113), 1.48 MiB | 22.23 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/clinicalml/gumbel-max-scm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv2brrpP2WlI"
      },
      "outputs": [],
      "source": [
        "#Enable importing code from parent directory\n",
        "import os, sys\n",
        "simulator_path = os.path.abspath('./gumbel-max-scm')\n",
        "sys.path.insert(1, simulator_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpUoCCBH_Ta8",
        "outputId": "797cf27b-e773-4c6c-c24e-9bef323a673d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymdptoolbox\n",
            "  Downloading pymdptoolbox-4.0-b3.zip (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pymdptoolbox) (1.10.1)\n",
            "Building wheels for collected packages: pymdptoolbox\n",
            "  Building wheel for pymdptoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymdptoolbox: filename=pymdptoolbox-4.0b3-py3-none-any.whl size=25657 sha256=cc7022cc7c8375fdd6dc285ed800d0eee464ead18bed3ea0d4a0178d4053f349\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/e7/c7/d7abf9e309f3573a934fed2750c70bd75d9e9d901f7f16e183\n",
            "Successfully built pymdptoolbox\n",
            "Installing collected packages: pymdptoolbox\n",
            "Successfully installed pymdptoolbox-4.0b3\n"
          ]
        }
      ],
      "source": [
        "!pip install pymdptoolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VImiXcP9FkB"
      },
      "source": [
        "**IMPORTANT NOTE:** At this stage, to reproduce our experiments, one must modify line 38 of `gumbel-max-scm/sepsisSimDiabetes/DataGenerator.py` so that it reads:\n",
        "\n",
        "```\n",
        "mdp = MDP(init_state_idx=%state%,\n",
        "          policy_array=policy, policy_idx_type=policy_idx_type,\n",
        "          p_diabetes=p_diabetes)\n",
        "\n",
        "```\n",
        "\n",
        "We have essentially set the initial state to a fixed value so that we may estimate the Q-function from that state. Additionally, line 58 of the same file must be modified to:\n",
        "\n",
        "```\n",
        "mdp.state = mdp.get_new_state(state_idx = %state%)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orCP8G0C3DCb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cf.counterfactual as cf\n",
        "import cf.utils as utils\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import itertools as it\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from scipy.linalg import block_diag\n",
        "\n",
        "# Sepsis Simulator code\n",
        "from sepsisSimDiabetes.State import State\n",
        "from sepsisSimDiabetes.Action import Action\n",
        "from sepsisSimDiabetes.DataGenerator import DataGenerator\n",
        "import sepsisSimDiabetes.MDP as simulator\n",
        "\n",
        "import mdptoolboxSrc.mdp as mdptools\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zt7QLKc3iQ7"
      },
      "source": [
        "# Set up Variables and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH6SMENMD-SV"
      },
      "source": [
        "Code taken from [Oberst and Sontag](https://github.com/clinicalml/gumbel-max-scm/blob/master/plots-main-paper.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gSS-DPD50f"
      },
      "source": [
        "Set up important variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTLB4gBp4Ww-"
      },
      "outputs": [],
      "source": [
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "NSIMSAMPS = 100000  # Samples to draw from the simulator\n",
        "NSTEPS = 20  # Max length of each trajectory\n",
        "NCFSAMPS = 5  # Counterfactual Samples per observed sample\n",
        "DISCOUNT_Pol = 0.99 # Used for computing optimal policies\n",
        "DISCOUNT = 1 # Used for computing actual reward\n",
        "PHYS_EPSILON = 0.05 # Used for sampling using physician pol as eps greedy\n",
        "\n",
        "# Option 1: Use bootstrapping w/replacement on the original NSIMSAMPS to estimate errors\n",
        "USE_BOOSTRAP=True\n",
        "N_BOOTSTRAP = 100\n",
        "\n",
        "# Option 2: Use repeated sampling (i.e., NSIMSAMPS fresh simulations each time) to get error bars;\n",
        "# This is done in the appendix of the paper, but not in the main paper\n",
        "N_REPEAT_SAMPLING = 1\n",
        "\n",
        "# These are properties of the simulator, do not change\n",
        "n_actions = Action.NUM_ACTIONS_TOTAL\n",
        "n_components = 2\n",
        "\n",
        "# These are added as absorbing states\n",
        "n_states_abs = State.NUM_OBS_STATES + 2\n",
        "discStateIdx = n_states_abs - 1\n",
        "deadStateIdx = n_states_abs - 2\n",
        "\n",
        "# Number of runs for calculating MSE\n",
        "RUNS = 20\n",
        "# Number of episodes over which we average an OPE estimate\n",
        "N = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUORk1KEedLa"
      },
      "outputs": [],
      "source": [
        "hr_state_mapping = ['Low', 'Normal', 'High']\n",
        "sbp_state_mapping = ['Low', 'Normal', 'High']\n",
        "o2_state_mapping = ['Low', 'Normal']\n",
        "glu_state_mapping = ['Very Low', 'Low', 'Normal', 'High', 'Very High']\n",
        "abx_state_mapping = ['Off', 'On']\n",
        "vaso_state_mapping = ['Off', 'On']\n",
        "vent_state_mapping = ['Off', 'On']\n",
        "diab_state_mapping = ['No', 'Yes']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2A-p4iXLLVn"
      },
      "source": [
        "Set up base for behaviour and evaluation policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmXqCQquBR-b"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"gumbel-max-scm/data/diab_txr_mats-replication.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"gumbel-max-scm/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHSM6i7nAJwg"
      },
      "outputs": [],
      "source": [
        "# Get the transition and reward matrix from file\n",
        "with open(\"gumbel-max-scm/data/diab_txr_mats-replication.pkl\", \"rb\") as f:\n",
        "    mdict = pickle.load(f)\n",
        "\n",
        "tx_mat = mdict[\"tx_mat\"]\n",
        "r_mat = mdict[\"r_mat\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL5_-lEZBxh6"
      },
      "outputs": [],
      "source": [
        "from scipy.linalg import block_diag\n",
        "\n",
        "tx_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
        "r_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
        "\n",
        "# Easily accessible variables\n",
        "A = n_actions\n",
        "S = State.NUM_FULL_STATES\n",
        "\n",
        "for a in range(n_actions):\n",
        "    tx_mat_full[a, ...] = block_diag(tx_mat[0, a, ...], tx_mat[1, a,...])\n",
        "    r_mat_full[a, ...] = block_diag(r_mat[0, a, ...], r_mat[1, a, ...])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc3sZU3pB-xp"
      },
      "outputs": [],
      "source": [
        "fullMDP = cf.MatrixMDP(tx_mat_full, r_mat_full)\n",
        "fullPol = fullMDP.policyIteration(discount=DISCOUNT_Pol, eval_type=1)\n",
        "\n",
        "#The behavior policy is the fully random policy\n",
        "randPol = np.ones(fullPol.shape)/(fullPol.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqM8gH597gjk",
        "outputId": "22bf5c44-9cac-4e5c-ab61-b3a3623877c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1440, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "#We want the expected reward of starting in a state and taking an action\n",
        "R = np.swapaxes(np.mean(r_mat_full, axis=-1), 0, 1)\n",
        "R.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOSZ8n6_R24G"
      },
      "outputs": [],
      "source": [
        "#To handle -1 states and -1 actions\n",
        "def pad_policy(policy, val=1):\n",
        "  #Add a column of zeroes to the end\n",
        "  policy = np.concatenate((policy, np.full((policy.shape[0], 1), val)), axis=1)\n",
        "  #Add a row of zeroes at the end\n",
        "  policy = np.concatenate((policy, np.full((1, policy.shape[1]), val)), axis=0)\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W3Fj_q5EADA"
      },
      "source": [
        "# Load repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY1b2UIxA9Ds",
        "outputId": "d369aae5-163a-43c8-e306-1b02e097488c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Factored-Action-Spaces-for-OPE'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 93 (delta 20), reused 71 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (93/93), 2.94 MiB | 19.32 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ai4ai-lab/Factored-Action-Spaces-for-OPE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4etBTF2ubmdb"
      },
      "outputs": [],
      "source": [
        "#Enable importing code from parent directory\n",
        "import os, sys\n",
        "main_folder = os.path.abspath('./Factored-Action-Spaces-for-OPE')\n",
        "sys.path.insert(1, main_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SYkHKlIMRj8"
      },
      "source": [
        "# From Patient State 136, With Diabetes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuOvmq-cMxFP"
      },
      "outputs": [],
      "source": [
        "#The patient has diabetes\n",
        "PROB_DIAB = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFB9lbC6M2Ks"
      },
      "source": [
        "### State Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrzAryBORmvz",
        "outputId": "815fed15-cd9c-4ea5-b3d6-7bf73ed17329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 2 0 0 0]\n",
            "Heart Rate: Low\n",
            "Systolic Blood Pressure: Normal\n",
            "Percent Oxygen: Normal\n",
            "Glucose Level: Normal\n",
            "Antibiotics: Off\n",
            "Vasopressors: Off\n",
            "Ventilator: Off\n",
            "Diabetes: 1\n"
          ]
        }
      ],
      "source": [
        "#Instantiate a state based on the idx and get the state vector\n",
        "testState = State(state_idx = 136, diabetic_idx=1)\n",
        "vec = testState.get_state_vector()\n",
        "\n",
        "print(vec)\n",
        "\n",
        "print(f'Heart Rate: {hr_state_mapping[vec[0]]}')\n",
        "print(f'Systolic Blood Pressure: {sbp_state_mapping[vec[1]]}')\n",
        "print(f'Percent Oxygen: {o2_state_mapping[vec[2]]}')\n",
        "print(f'Glucose Level: {glu_state_mapping[vec[3]]}')\n",
        "print(f'Antibiotics: {abx_state_mapping[vec[4]]}')\n",
        "print(f'Vasopressors: {vaso_state_mapping[vec[5]]}')\n",
        "print(f'Ventilator: {vent_state_mapping[vec[6]]}')\n",
        "print(f'Diabetes: {testState.diabetic_idx}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPF8wN8gNZEK"
      },
      "source": [
        "### Generate Data From Behaviour Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGbGvpAiLOr0"
      },
      "source": [
        "Run the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSAmbkYoDQRe"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=randPol, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bhLMTrLQ__"
      },
      "source": [
        "Convert data into array format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsk5poscGNBH",
        "outputId": "965bb806-1a33-4232-a250-4b98e82afa31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_b = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSY3QNm8wgKb",
        "outputId": "76d2a358-adca-4e1f-d0b0-f02aea634627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[  0. 136.   0.  -1. 168.]\n",
            "  [  1. 168.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   3.  -1. 227.]\n",
            "  [  1. 227.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.   0. 223.]\n",
            "  [  1. 223.   3.   0. 219.]\n",
            "  [  2. 219.   1.   0. 218.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0. 136.   2.   0. 377.]\n",
            "  [  1. 377.   4.   0. 372.]\n",
            "  [  2. 372.   7.   0. 463.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   5.   0. 222.]\n",
            "  [  1. 222.   1.   0. 218.]\n",
            "  [  2. 218.   7.  -1. 231.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.  -1. 231.]\n",
            "  [  1. 231.  -1.   0.  -1.]\n",
            "  [  2.  -1.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]]\n"
          ]
        }
      ],
      "source": [
        "print(nf_tr_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcRSeGkJITQZ"
      },
      "outputs": [],
      "source": [
        "randPol = pad_policy(randPol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljqSoSVUI_7u"
      },
      "source": [
        "### Varying Episodes $\\epsilon_{e} = 0.4$ (Policy Divergence $4.8^{20}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WGmgmigI_7v"
      },
      "source": [
        "Set up evaluation policy, generate data and convert into factored format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3Kvf7JwI_7v"
      },
      "outputs": [],
      "source": [
        "EVAL_EPSILON = 0.4\n",
        "\n",
        "evalPolSoft = np.copy(fullPol)\n",
        "evalPolSoft[evalPolSoft == 1] = 1 - EVAL_EPSILON\n",
        "evalPolSoft[evalPolSoft == 0] = EVAL_EPSILON / (n_actions - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT2KzVpCI_7v",
        "outputId": "0ce16801-1fbb-4fc1-b46a-2242e168ef6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ],
      "source": [
        "# Calculate policy divergence from Voloshin et al.\n",
        "D = 0\n",
        "for state in range(randPol.shape[0] - 1):\n",
        "    for action in range(randPol.shape[1] - 1):\n",
        "        difference = evalPolSoft[state, action]/randPol[state, action]\n",
        "        D = max(D, difference)\n",
        "print(D)\n",
        "shorter_D = round(D,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TreqvybTI_7w"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=evalPolSoft, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69P47oFMI_7w",
        "outputId": "5ded6ed2-2ebb-4290-bcfa-6d37d4b71d6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_e = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pHiTPTj1VYX",
        "outputId": "1373e0ae-7573-4685-b9a5-272a51d669a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[  0. 136.   0.   0. 136.]\n",
            "  [  1. 136.   2.   0.  57.]\n",
            "  [  2.  57.   3.  -1. 227.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0.  57.]\n",
            "  [  1.  57.   4.  -1.  68.]\n",
            "  [  2.  68.  -1.   0.  -1.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0. 449.]\n",
            "  [  1. 449.   5.   0. 462.]\n",
            "  [  2. 462.   7.   0. 471.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0. 136.   2.   0. 145.]\n",
            "  [  1. 145.   3.   0. 147.]\n",
            "  [  2. 147.   2.   0.  57.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   7.   0. 223.]\n",
            "  [  1. 223.   6.   0. 221.]\n",
            "  [  2. 221.   4.   0. 132.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]\n",
            "\n",
            " [[  0. 136.   2.   0.  57.]\n",
            "  [  1.  57.   3.   0. 147.]\n",
            "  [  2. 147.   2.   0. 145.]\n",
            "  ...\n",
            "  [ 17.  -1.  -1.   0.  -1.]\n",
            "  [ 18.  -1.  -1.   0.  -1.]\n",
            "  [ 19.  -1.  -1.   0.  -1.]]]\n"
          ]
        }
      ],
      "source": [
        "print(nf_tr_e)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import policy_estimators as pe\n",
        "\n",
        "#Obtain on policy estimate\n",
        "on_policy_estimate = pe.on_policy_Q_estimate(nf_tr_e, DISCOUNT_Pol)"
      ],
      "metadata": {
        "id": "sPMOQPfzh102"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plan\n",
        "\n",
        "- Given data\n",
        "- Objective: lower MSE with respect to on-policy estimate\n",
        "\n",
        "- Evaluate on-policy estimate (true value)\n",
        "- COnstruct a neural network to accept the action and state and output the decomposed policy values and decomposed rewards (2D outputs)\n",
        "- Pass data through network in batches(runs) and based on this find the MSE\n",
        "- Attempt to backpropagate through the network and ultimately obtain the best mapping."
      ],
      "metadata": {
        "id": "D4ZRTG0U_W8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "true_val = torch.as_tensor(on_policy_estimate).to(device)"
      ],
      "metadata": {
        "id": "kbi087bSAVNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b610e9-2835-47e5-cfa6-c1375393d711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FactorNetwork(nn.Module):\n",
        "    def __init__(self, D):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(2, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 3*D),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "f-OIrdKuBdu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, loss_fn, optimizer, discount_factor):\n",
        "    BATCH_SIZE = 1000\n",
        "    train_data = nf_tr_b.reshape((-1, BATCH_SIZE, NSTEPS, 5))\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for run in range(train_data.shape[0]):\n",
        "      loss = torch.zeros((1), dtype=torch.float32).to(device)\n",
        "      batch = train_data[run, :, :, :]\n",
        "      for n in range(BATCH_SIZE):\n",
        "        episode = batch[n, :, :]\n",
        "        #Filter out -1 states and actions\n",
        "        episode = episode[episode[:, 2] != -1, :]\n",
        "        states_and_actions = torch.as_tensor(episode[:, 1:3], dtype=torch.float32).to(device)\n",
        "        # Compute prediction and loss\n",
        "        factored_pol_reward = model(states_and_actions) #Use predictions from network to calculate OPE estimates\n",
        "        D = list(factored_pol_reward.size())[-1]//3\n",
        "        factored_pi_b = factored_pol_reward[:, :D]\n",
        "        factored_pi_e = factored_pol_reward[:, D:2*D]\n",
        "        factored_reward = factored_pol_reward[:, 2*D:]\n",
        "\n",
        "        fn = nn.ReLU()\n",
        "        #Penalty for behaviour policy values < 1\n",
        "        penalty1 = torch.sum(fn(torch.neg(factored_pi_b))).to(device)\n",
        "        #Penalty for behaviour policy values < 1\n",
        "        penalty2 = torch.sum(fn(torch.neg(factored_pi_e))).to(device)\n",
        "        total_penalty1 = torch.add(penalty1, penalty2)\n",
        "\n",
        "        #Penalty for policies not summing to 1\n",
        "        penalty3 = torch.abs( torch.sub( torch.ones((1), dtype=torch.float32).to(device), torch.sum(factored_pi_b) ) )\n",
        "        penalty4 = torch.abs( torch.sub( torch.ones((1), dtype=torch.float32).to(device), torch.sum(factored_pi_e) ) )\n",
        "        total_penalty2 = torch.add(penalty3, penalty4)\n",
        "\n",
        "        total_penalty = torch.add(total_penalty1, total_penalty2).div(episode.shape[0])\n",
        "\n",
        "        pointwise_IS_ratios = torch.div(factored_pi_e, factored_pi_b)\n",
        "        IS_ratios = torch.prod(pointwise_IS_ratios, 0)\n",
        "\n",
        "        times = torch.as_tensor(np.repeat(np.expand_dims(episode[:, 0], axis=1), D, axis=1)).to(device)\n",
        "        # Per-trajectory returns (discounted cumulative rewards)\n",
        "        gamma = torch.full(times.shape, discount_factor).to(device)\n",
        "        G = torch.mul(factored_reward, torch.pow(gamma, times)).sum()\n",
        "\n",
        "        loss.to(device)\n",
        "        loss = torch.add(loss, torch.add(loss_fn(true_val, G), total_penalty).div(BATCH_SIZE))\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if run % 10 == 0:\n",
        "          loss, current = loss.item(), (run + 1) * BATCH_SIZE\n",
        "          print(f\"loss: {loss:>7f}  [{run:>5d} /{train_data.shape[0]:>5d}]\")"
      ],
      "metadata": {
        "id": "nk9h3RO7LptX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = FactorNetwork(2).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model1.parameters(), lr=0.00001)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(model1, loss_fn, optimizer, DISCOUNT_Pol)\n",
        "\n",
        "#Test it out\n",
        "model1.eval()\n",
        "test = torch.tensor([136.0, 0.0]).to(device)\n",
        "model1(test)"
      ],
      "metadata": {
        "id": "yMy56QYA4tjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2757e3-4a5a-4fd4-e6d0-37a8639e0da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 1.335135  [    0 /  100]\n",
            "loss: 1.278470  [   10 /  100]\n",
            "loss: 1.342532  [   20 /  100]\n",
            "loss: 1.285794  [   30 /  100]\n",
            "loss: 1.259601  [   40 /  100]\n",
            "loss: 1.300566  [   50 /  100]\n",
            "loss: 1.274568  [   60 /  100]\n",
            "loss: 1.278864  [   70 /  100]\n",
            "loss: 1.249542  [   80 /  100]\n",
            "loss: 1.270595  [   90 /  100]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.294299  [    0 /  100]\n",
            "loss: 1.236093  [   10 /  100]\n",
            "loss: 1.300161  [   20 /  100]\n",
            "loss: 1.245162  [   30 /  100]\n",
            "loss: 1.213984  [   40 /  100]\n",
            "loss: 1.258779  [   50 /  100]\n",
            "loss: 1.231544  [   60 /  100]\n",
            "loss: 1.233908  [   70 /  100]\n",
            "loss: 1.208391  [   80 /  100]\n",
            "loss: 1.228957  [   90 /  100]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.255375  [    0 /  100]\n",
            "loss: 1.195838  [   10 /  100]\n",
            "loss: 1.259844  [   20 /  100]\n",
            "loss: 1.206623  [   30 /  100]\n",
            "loss: 1.170749  [   40 /  100]\n",
            "loss: 1.219081  [   50 /  100]\n",
            "loss: 1.190683  [   60 /  100]\n",
            "loss: 1.191433  [   70 /  100]\n",
            "loss: 1.169471  [   80 /  100]\n",
            "loss: 1.189655  [   90 /  100]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.218470  [    0 /  100]\n",
            "loss: 1.157642  [   10 /  100]\n",
            "loss: 1.221688  [   20 /  100]\n",
            "loss: 1.170391  [   30 /  100]\n",
            "loss: 1.129911  [   40 /  100]\n",
            "loss: 1.181528  [   50 /  100]\n",
            "loss: 1.152105  [   60 /  100]\n",
            "loss: 1.151406  [   70 /  100]\n",
            "loss: 1.132719  [   80 /  100]\n",
            "loss: 1.152509  [   90 /  100]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.183672  [    0 /  100]\n",
            "loss: 1.121560  [   10 /  100]\n",
            "loss: 1.185732  [   20 /  100]\n",
            "loss: 1.136277  [   30 /  100]\n",
            "loss: 1.091528  [   40 /  100]\n",
            "loss: 1.146106  [   50 /  100]\n",
            "loss: 1.116071  [   60 /  100]\n",
            "loss: 1.113971  [   70 /  100]\n",
            "loss: 1.098943  [   80 /  100]\n",
            "loss: 1.118600  [   90 /  100]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.5042,  0.0189,  0.1691,  0.1965,  0.6506, -0.6988], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = FactorNetwork(3).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model2.parameters(), lr=0.00001)\n",
        "\n",
        "epochs = 40\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(model2, loss_fn, optimizer, DISCOUNT_Pol)\n",
        "\n",
        "#Test it out\n",
        "model2.eval()\n",
        "test = torch.tensor([136.0, 0.0]).to(device)\n",
        "model2(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5idY5vb0tY6x",
        "outputId": "734e0e0d-9d29-4fe9-ffac-458f47e87508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 121.918434  [    0 /  100]\n",
            "loss: 85.112808  [   10 /  100]\n",
            "loss: 68.127640  [   20 /  100]\n",
            "loss: 51.087219  [   30 /  100]\n",
            "loss: 42.952618  [   40 /  100]\n",
            "loss: 38.361416  [   50 /  100]\n",
            "loss: 34.953259  [   60 /  100]\n",
            "loss: 32.426785  [   70 /  100]\n",
            "loss: 30.143805  [   80 /  100]\n",
            "loss: 28.798235  [   90 /  100]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 27.612373  [    0 /  100]\n",
            "loss: 26.265516  [   10 /  100]\n",
            "loss: 25.568386  [   20 /  100]\n",
            "loss: 24.301170  [   30 /  100]\n",
            "loss: 23.491777  [   40 /  100]\n",
            "loss: 22.669216  [   50 /  100]\n",
            "loss: 21.819134  [   60 /  100]\n",
            "loss: 20.983740  [   70 /  100]\n",
            "loss: 20.019804  [   80 /  100]\n",
            "loss: 19.277946  [   90 /  100]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 18.544365  [    0 /  100]\n",
            "loss: 17.535997  [   10 /  100]\n",
            "loss: 16.957306  [   20 /  100]\n",
            "loss: 15.855524  [   30 /  100]\n",
            "loss: 15.214669  [   40 /  100]\n",
            "loss: 14.591391  [   50 /  100]\n",
            "loss: 13.922743  [   60 /  100]\n",
            "loss: 13.276759  [   70 /  100]\n",
            "loss: 12.482839  [   80 /  100]\n",
            "loss: 11.890295  [   90 /  100]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 11.336457  [    0 /  100]\n",
            "loss: 10.465505  [   10 /  100]\n",
            "loss: 10.014530  [   20 /  100]\n",
            "loss: 9.231583  [   30 /  100]\n",
            "loss: 8.782990  [   40 /  100]\n",
            "loss: 8.331164  [   50 /  100]\n",
            "loss: 7.854633  [   60 /  100]\n",
            "loss: 7.429786  [   70 /  100]\n",
            "loss: 6.933214  [   80 /  100]\n",
            "loss: 6.669063  [   90 /  100]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 6.504627  [    0 /  100]\n",
            "loss: 6.258019  [   10 /  100]\n",
            "loss: 6.180474  [   20 /  100]\n",
            "loss: 6.000417  [   30 /  100]\n",
            "loss: 5.879261  [   40 /  100]\n",
            "loss: 5.747662  [   50 /  100]\n",
            "loss: 5.596647  [   60 /  100]\n",
            "loss: 5.506503  [   70 /  100]\n",
            "loss: 5.343366  [   80 /  100]\n",
            "loss: 5.177441  [   90 /  100]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 5.079721  [    0 /  100]\n",
            "loss: 4.890721  [   10 /  100]\n",
            "loss: 4.813694  [   20 /  100]\n",
            "loss: 4.635184  [   30 /  100]\n",
            "loss: 4.515204  [   40 /  100]\n",
            "loss: 4.385600  [   50 /  100]\n",
            "loss: 4.237308  [   60 /  100]\n",
            "loss: 4.146215  [   70 /  100]\n",
            "loss: 3.983556  [   80 /  100]\n",
            "loss: 3.821178  [   90 /  100]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 3.725199  [    0 /  100]\n",
            "loss: 3.536533  [   10 /  100]\n",
            "loss: 3.462625  [   20 /  100]\n",
            "loss: 3.286298  [   30 /  100]\n",
            "loss: 3.190655  [   40 /  100]\n",
            "loss: 3.127539  [   50 /  100]\n",
            "loss: 3.066408  [   60 /  100]\n",
            "loss: 3.049577  [   70 /  100]\n",
            "loss: 2.964248  [   80 /  100]\n",
            "loss: 2.901386  [   90 /  100]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.909431  [    0 /  100]\n",
            "loss: 2.787107  [   10 /  100]\n",
            "loss: 2.827791  [   20 /  100]\n",
            "loss: 2.707623  [   30 /  100]\n",
            "loss: 2.677095  [   40 /  100]\n",
            "loss: 2.649693  [   50 /  100]\n",
            "loss: 2.597620  [   60 /  100]\n",
            "loss: 2.583298  [   70 /  100]\n",
            "loss: 2.495107  [   80 /  100]\n",
            "loss: 2.439869  [   90 /  100]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.455675  [    0 /  100]\n",
            "loss: 2.329422  [   10 /  100]\n",
            "loss: 2.382262  [   20 /  100]\n",
            "loss: 2.259699  [   30 /  100]\n",
            "loss: 2.235167  [   40 /  100]\n",
            "loss: 2.213591  [   50 /  100]\n",
            "loss: 2.165130  [   60 /  100]\n",
            "loss: 2.158402  [   70 /  100]\n",
            "loss: 2.074349  [   80 /  100]\n",
            "loss: 2.023122  [   90 /  100]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.042727  [    0 /  100]\n",
            "loss: 1.921012  [   10 /  100]\n",
            "loss: 1.976511  [   20 /  100]\n",
            "loss: 1.856576  [   30 /  100]\n",
            "loss: 1.834431  [   40 /  100]\n",
            "loss: 1.814650  [   50 /  100]\n",
            "loss: 1.766126  [   60 /  100]\n",
            "loss: 1.763632  [   70 /  100]\n",
            "loss: 1.677955  [   80 /  100]\n",
            "loss: 1.627429  [   90 /  100]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.647595  [    0 /  100]\n",
            "loss: 1.523104  [   10 /  100]\n",
            "loss: 1.583068  [   20 /  100]\n",
            "loss: 1.458512  [   30 /  100]\n",
            "loss: 1.436874  [   40 /  100]\n",
            "loss: 1.417626  [   50 /  100]\n",
            "loss: 1.368106  [   60 /  100]\n",
            "loss: 1.368177  [   70 /  100]\n",
            "loss: 1.279924  [   80 /  100]\n",
            "loss: 1.229819  [   90 /  100]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.251353  [    0 /  100]\n",
            "loss: 1.122464  [   10 /  100]\n",
            "loss: 1.188483  [   20 /  100]\n",
            "loss: 1.058722  [   30 /  100]\n",
            "loss: 1.037203  [   40 /  100]\n",
            "loss: 1.018685  [   50 /  100]\n",
            "loss: 0.982921  [   60 /  100]\n",
            "loss: 1.016649  [   70 /  100]\n",
            "loss: 0.953438  [   80 /  100]\n",
            "loss: 0.948362  [   90 /  100]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.012268  [    0 /  100]\n",
            "loss: 0.904872  [   10 /  100]\n",
            "loss: 1.027645  [   20 /  100]\n",
            "loss: 0.913153  [   30 /  100]\n",
            "loss: 0.930554  [   40 /  100]\n",
            "loss: 0.953462  [   50 /  100]\n",
            "loss: 0.944571  [   60 /  100]\n",
            "loss: 0.979289  [   70 /  100]\n",
            "loss: 0.910010  [   80 /  100]\n",
            "loss: 0.914985  [   90 /  100]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.981965  [    0 /  100]\n",
            "loss: 0.871923  [   10 /  100]\n",
            "loss: 0.998338  [   20 /  100]\n",
            "loss: 0.881870  [   30 /  100]\n",
            "loss: 0.899186  [   40 /  100]\n",
            "loss: 0.922983  [   50 /  100]\n",
            "loss: 0.915325  [   60 /  100]\n",
            "loss: 0.948657  [   70 /  100]\n",
            "loss: 0.879043  [   80 /  100]\n",
            "loss: 0.886205  [   90 /  100]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.953199  [    0 /  100]\n",
            "loss: 0.843659  [   10 /  100]\n",
            "loss: 0.969634  [   20 /  100]\n",
            "loss: 0.853497  [   30 /  100]\n",
            "loss: 0.870528  [   40 /  100]\n",
            "loss: 0.894626  [   50 /  100]\n",
            "loss: 0.887359  [   60 /  100]\n",
            "loss: 0.919534  [   70 /  100]\n",
            "loss: 0.851694  [   80 /  100]\n",
            "loss: 0.858782  [   90 /  100]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.924836  [    0 /  100]\n",
            "loss: 0.817720  [   10 /  100]\n",
            "loss: 0.941314  [   20 /  100]\n",
            "loss: 0.827043  [   30 /  100]\n",
            "loss: 0.843582  [   40 /  100]\n",
            "loss: 0.867567  [   50 /  100]\n",
            "loss: 0.860646  [   60 /  100]\n",
            "loss: 0.891378  [   70 /  100]\n",
            "loss: 0.825806  [   80 /  100]\n",
            "loss: 0.832422  [   90 /  100]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.897460  [    0 /  100]\n",
            "loss: 0.792233  [   10 /  100]\n",
            "loss: 0.913735  [   20 /  100]\n",
            "loss: 0.801506  [   30 /  100]\n",
            "loss: 0.817767  [   40 /  100]\n",
            "loss: 0.841337  [   50 /  100]\n",
            "loss: 0.834844  [   60 /  100]\n",
            "loss: 0.864265  [   70 /  100]\n",
            "loss: 0.800339  [   80 /  100]\n",
            "loss: 0.807087  [   90 /  100]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.871001  [    0 /  100]\n",
            "loss: 0.768481  [   10 /  100]\n",
            "loss: 0.887405  [   20 /  100]\n",
            "loss: 0.777339  [   30 /  100]\n",
            "loss: 0.794059  [   40 /  100]\n",
            "loss: 0.817732  [   50 /  100]\n",
            "loss: 0.812271  [   60 /  100]\n",
            "loss: 0.840888  [   70 /  100]\n",
            "loss: 0.780782  [   80 /  100]\n",
            "loss: 0.786410  [   90 /  100]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.849984  [    0 /  100]\n",
            "loss: 0.748231  [   10 /  100]\n",
            "loss: 0.867197  [   20 /  100]\n",
            "loss: 0.758897  [   30 /  100]\n",
            "loss: 0.775405  [   40 /  100]\n",
            "loss: 0.799190  [   50 /  100]\n",
            "loss: 0.794144  [   60 /  100]\n",
            "loss: 0.821737  [   70 /  100]\n",
            "loss: 0.763022  [   80 /  100]\n",
            "loss: 0.768919  [   90 /  100]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.831481  [    0 /  100]\n",
            "loss: 0.731431  [   10 /  100]\n",
            "loss: 0.848673  [   20 /  100]\n",
            "loss: 0.741979  [   30 /  100]\n",
            "loss: 0.758336  [   40 /  100]\n",
            "loss: 0.781984  [   50 /  100]\n",
            "loss: 0.777164  [   60 /  100]\n",
            "loss: 0.804022  [   70 /  100]\n",
            "loss: 0.746806  [   80 /  100]\n",
            "loss: 0.752552  [   90 /  100]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.814049  [    0 /  100]\n",
            "loss: 0.716969  [   10 /  100]\n",
            "loss: 0.830956  [   20 /  100]\n",
            "loss: 0.726086  [   30 /  100]\n",
            "loss: 0.742465  [   40 /  100]\n",
            "loss: 0.765722  [   50 /  100]\n",
            "loss: 0.761083  [   60 /  100]\n",
            "loss: 0.787159  [   70 /  100]\n",
            "loss: 0.730662  [   80 /  100]\n",
            "loss: 0.736935  [   90 /  100]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.797532  [    0 /  100]\n",
            "loss: 0.702116  [   10 /  100]\n",
            "loss: 0.814266  [   20 /  100]\n",
            "loss: 0.710775  [   30 /  100]\n",
            "loss: 0.726771  [   40 /  100]\n",
            "loss: 0.749985  [   50 /  100]\n",
            "loss: 0.745528  [   60 /  100]\n",
            "loss: 0.770940  [   70 /  100]\n",
            "loss: 0.715686  [   80 /  100]\n",
            "loss: 0.721596  [   90 /  100]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.781697  [    0 /  100]\n",
            "loss: 0.685714  [   10 /  100]\n",
            "loss: 0.798395  [   20 /  100]\n",
            "loss: 0.695809  [   30 /  100]\n",
            "loss: 0.711704  [   40 /  100]\n",
            "loss: 0.734882  [   50 /  100]\n",
            "loss: 0.730624  [   60 /  100]\n",
            "loss: 0.755201  [   70 /  100]\n",
            "loss: 0.699197  [   80 /  100]\n",
            "loss: 0.706894  [   90 /  100]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.766271  [    0 /  100]\n",
            "loss: 0.671650  [   10 /  100]\n",
            "loss: 0.782966  [   20 /  100]\n",
            "loss: 0.681425  [   30 /  100]\n",
            "loss: 0.697031  [   40 /  100]\n",
            "loss: 0.720176  [   50 /  100]\n",
            "loss: 0.715934  [   60 /  100]\n",
            "loss: 0.739953  [   70 /  100]\n",
            "loss: 0.684314  [   80 /  100]\n",
            "loss: 0.692731  [   90 /  100]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.751128  [    0 /  100]\n",
            "loss: 0.657832  [   10 /  100]\n",
            "loss: 0.768024  [   20 /  100]\n",
            "loss: 0.667546  [   30 /  100]\n",
            "loss: 0.682873  [   40 /  100]\n",
            "loss: 0.705999  [   50 /  100]\n",
            "loss: 0.701870  [   60 /  100]\n",
            "loss: 0.725260  [   70 /  100]\n",
            "loss: 0.670438  [   80 /  100]\n",
            "loss: 0.678862  [   90 /  100]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.736480  [    0 /  100]\n",
            "loss: 0.645331  [   10 /  100]\n",
            "loss: 0.753492  [   20 /  100]\n",
            "loss: 0.654075  [   30 /  100]\n",
            "loss: 0.668908  [   40 /  100]\n",
            "loss: 0.692161  [   50 /  100]\n",
            "loss: 0.688078  [   60 /  100]\n",
            "loss: 0.711401  [   70 /  100]\n",
            "loss: 0.656990  [   80 /  100]\n",
            "loss: 0.666657  [   90 /  100]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.722973  [    0 /  100]\n",
            "loss: 0.634174  [   10 /  100]\n",
            "loss: 0.740310  [   20 /  100]\n",
            "loss: 0.643199  [   30 /  100]\n",
            "loss: 0.658096  [   40 /  100]\n",
            "loss: 0.681357  [   50 /  100]\n",
            "loss: 0.677172  [   60 /  100]\n",
            "loss: 0.700410  [   70 /  100]\n",
            "loss: 0.647030  [   80 /  100]\n",
            "loss: 0.656110  [   90 /  100]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.711432  [    0 /  100]\n",
            "loss: 0.624095  [   10 /  100]\n",
            "loss: 0.729244  [   20 /  100]\n",
            "loss: 0.633589  [   30 /  100]\n",
            "loss: 0.648822  [   40 /  100]\n",
            "loss: 0.671231  [   50 /  100]\n",
            "loss: 0.666791  [   60 /  100]\n",
            "loss: 0.689972  [   70 /  100]\n",
            "loss: 0.635695  [   80 /  100]\n",
            "loss: 0.645805  [   90 /  100]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.700567  [    0 /  100]\n",
            "loss: 0.615100  [   10 /  100]\n",
            "loss: 0.717681  [   20 /  100]\n",
            "loss: 0.623522  [   30 /  100]\n",
            "loss: 0.637820  [   40 /  100]\n",
            "loss: 0.660627  [   50 /  100]\n",
            "loss: 0.656514  [   60 /  100]\n",
            "loss: 0.678937  [   70 /  100]\n",
            "loss: 0.626014  [   80 /  100]\n",
            "loss: 0.636030  [   90 /  100]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.689622  [    0 /  100]\n",
            "loss: 0.605222  [   10 /  100]\n",
            "loss: 0.706616  [   20 /  100]\n",
            "loss: 0.614255  [   30 /  100]\n",
            "loss: 0.628117  [   40 /  100]\n",
            "loss: 0.650744  [   50 /  100]\n",
            "loss: 0.646496  [   60 /  100]\n",
            "loss: 0.668573  [   70 /  100]\n",
            "loss: 0.617843  [   80 /  100]\n",
            "loss: 0.626274  [   90 /  100]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.679082  [    0 /  100]\n",
            "loss: 0.595883  [   10 /  100]\n",
            "loss: 0.695881  [   20 /  100]\n",
            "loss: 0.605018  [   30 /  100]\n",
            "loss: 0.618588  [   40 /  100]\n",
            "loss: 0.640984  [   50 /  100]\n",
            "loss: 0.636688  [   60 /  100]\n",
            "loss: 0.658559  [   70 /  100]\n",
            "loss: 0.608681  [   80 /  100]\n",
            "loss: 0.616767  [   90 /  100]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.668787  [    0 /  100]\n",
            "loss: 0.586800  [   10 /  100]\n",
            "loss: 0.685664  [   20 /  100]\n",
            "loss: 0.595435  [   30 /  100]\n",
            "loss: 0.608829  [   40 /  100]\n",
            "loss: 0.630798  [   50 /  100]\n",
            "loss: 0.626738  [   60 /  100]\n",
            "loss: 0.648018  [   70 /  100]\n",
            "loss: 0.597691  [   80 /  100]\n",
            "loss: 0.607103  [   90 /  100]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.658331  [    0 /  100]\n",
            "loss: 0.577988  [   10 /  100]\n",
            "loss: 0.674601  [   20 /  100]\n",
            "loss: 0.586542  [   30 /  100]\n",
            "loss: 0.599601  [   40 /  100]\n",
            "loss: 0.621563  [   50 /  100]\n",
            "loss: 0.617391  [   60 /  100]\n",
            "loss: 0.638511  [   70 /  100]\n",
            "loss: 0.590884  [   80 /  100]\n",
            "loss: 0.598231  [   90 /  100]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.648986  [    0 /  100]\n",
            "loss: 0.569085  [   10 /  100]\n",
            "loss: 0.664793  [   20 /  100]\n",
            "loss: 0.577462  [   30 /  100]\n",
            "loss: 0.590533  [   40 /  100]\n",
            "loss: 0.612152  [   50 /  100]\n",
            "loss: 0.608091  [   60 /  100]\n",
            "loss: 0.628704  [   70 /  100]\n",
            "loss: 0.581106  [   80 /  100]\n",
            "loss: 0.589202  [   90 /  100]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.638773  [    0 /  100]\n",
            "loss: 0.560306  [   10 /  100]\n",
            "loss: 0.655338  [   20 /  100]\n",
            "loss: 0.568707  [   30 /  100]\n",
            "loss: 0.581521  [   40 /  100]\n",
            "loss: 0.602979  [   50 /  100]\n",
            "loss: 0.598927  [   60 /  100]\n",
            "loss: 0.619200  [   70 /  100]\n",
            "loss: 0.572565  [   80 /  100]\n",
            "loss: 0.580366  [   90 /  100]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.629124  [    0 /  100]\n",
            "loss: 0.552343  [   10 /  100]\n",
            "loss: 0.645839  [   20 /  100]\n",
            "loss: 0.560123  [   30 /  100]\n",
            "loss: 0.572761  [   40 /  100]\n",
            "loss: 0.593989  [   50 /  100]\n",
            "loss: 0.589949  [   60 /  100]\n",
            "loss: 0.609920  [   70 /  100]\n",
            "loss: 0.563203  [   80 /  100]\n",
            "loss: 0.571661  [   90 /  100]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.619701  [    0 /  100]\n",
            "loss: 0.543960  [   10 /  100]\n",
            "loss: 0.636130  [   20 /  100]\n",
            "loss: 0.551669  [   30 /  100]\n",
            "loss: 0.564124  [   40 /  100]\n",
            "loss: 0.585101  [   50 /  100]\n",
            "loss: 0.581070  [   60 /  100]\n",
            "loss: 0.600834  [   70 /  100]\n",
            "loss: 0.554666  [   80 /  100]\n",
            "loss: 0.563097  [   90 /  100]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.610493  [    0 /  100]\n",
            "loss: 0.535268  [   10 /  100]\n",
            "loss: 0.626348  [   20 /  100]\n",
            "loss: 0.543341  [   30 /  100]\n",
            "loss: 0.555608  [   40 /  100]\n",
            "loss: 0.576333  [   50 /  100]\n",
            "loss: 0.572349  [   60 /  100]\n",
            "loss: 0.591773  [   70 /  100]\n",
            "loss: 0.546048  [   80 /  100]\n",
            "loss: 0.554674  [   90 /  100]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.601734  [    0 /  100]\n",
            "loss: 0.527683  [   10 /  100]\n",
            "loss: 0.616538  [   20 /  100]\n",
            "loss: 0.535245  [   30 /  100]\n",
            "loss: 0.547327  [   40 /  100]\n",
            "loss: 0.567777  [   50 /  100]\n",
            "loss: 0.563723  [   60 /  100]\n",
            "loss: 0.582866  [   70 /  100]\n",
            "loss: 0.539397  [   80 /  100]\n",
            "loss: 0.546336  [   90 /  100]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.592074  [    0 /  100]\n",
            "loss: 0.519428  [   10 /  100]\n",
            "loss: 0.607242  [   20 /  100]\n",
            "loss: 0.527249  [   30 /  100]\n",
            "loss: 0.539294  [   40 /  100]\n",
            "loss: 0.559573  [   50 /  100]\n",
            "loss: 0.555352  [   60 /  100]\n",
            "loss: 0.573839  [   70 /  100]\n",
            "loss: 0.529139  [   80 /  100]\n",
            "loss: 0.537915  [   90 /  100]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0074,  0.0374, -0.0400, -0.0485, -0.0236, -0.0033, -0.0809,  0.1385,\n",
              "         0.0095], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a new batch of behaviour data"
      ],
      "metadata": {
        "id": "reumIqKKv84J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_fWXVK6S4w"
      },
      "source": [
        "Run the data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in3w55oZ6S4w"
      },
      "outputs": [],
      "source": [
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=randPol, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJwI24gq6S4x"
      },
      "source": [
        "Convert data into array format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c39e13-76a3-4a9a-ea24-da5133329b6d",
        "id": "Wg7pZWVD6S4x"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_b2 = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_b2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OfB67uh8MpQ"
      },
      "source": [
        "### Varying Episodes $\\epsilon_{e} = 0.6$ (Policy Divergence $3.2^{20}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eivmp_wT8Mpa"
      },
      "source": [
        "Set up evaluation policy, generate data and convert into factored format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz883VeI8Mpb"
      },
      "outputs": [],
      "source": [
        "EVAL_EPSILON = 0.6\n",
        "\n",
        "evalPolSoft = np.copy(fullPol)\n",
        "evalPolSoft[evalPolSoft == 1] = 1 - EVAL_EPSILON\n",
        "evalPolSoft[evalPolSoft == 0] = EVAL_EPSILON / (n_actions - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gBUFwhp8Mpb",
        "outputId": "8e6413a9-0390-4640-af56-55011d36c945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.2\n"
          ]
        }
      ],
      "source": [
        "# Calculate policy divergence from Voloshin et al.\n",
        "D = 0\n",
        "for state in range(randPol.shape[0] - 1):\n",
        "    for action in range(randPol.shape[1] - 1):\n",
        "        difference = evalPolSoft[state, action]/randPol[state, action]\n",
        "        D = max(D, difference)\n",
        "print(D)\n",
        "shorter_D = round(D,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmvwbIo78Mpb"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=evalPolSoft, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dYMGWrU8Mpb",
        "outputId": "a1aefa20-2c9c-4995-a7cd-9ded408ebad2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_e = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IN1k2vEwNnw"
      },
      "outputs": [],
      "source": [
        "evalPolSoft = pad_policy(evalPolSoft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRv-tjzXnMiQ"
      },
      "source": [
        "### Varying Episodes $\\epsilon_{e} = 0.8$ (Policy Divergence $1.6^{20}$)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUL8VFPtnMie"
      },
      "source": [
        "Set up evaluation policy, generate data and convert into factored format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2leNN3vnMif"
      },
      "outputs": [],
      "source": [
        "EVAL_EPSILON = 0.8\n",
        "\n",
        "evalPolSoft = np.copy(fullPol)\n",
        "evalPolSoft[evalPolSoft == 1] = 1 - EVAL_EPSILON\n",
        "evalPolSoft[evalPolSoft == 0] = EVAL_EPSILON / (n_actions - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hj-jl1LnMif",
        "outputId": "c1f77ac8-b41c-427a-cccd-29b866a7080a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5999999999999996\n"
          ]
        }
      ],
      "source": [
        "# Calculate policy divergence from Voloshin et al.\n",
        "D = 0\n",
        "for state in range(randPol.shape[0] - 1):\n",
        "    for action in range(randPol.shape[1] - 1):\n",
        "        difference = evalPolSoft[state, action]/randPol[state, action]\n",
        "        D = max(D, difference)\n",
        "print(D)\n",
        "shorter_D = round(D,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCR8p6BjnMig"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=evalPolSoft, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTkLY_ywnMig",
        "outputId": "b427ede8-39a6-4cb7-e3c3-f4138bb72512"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_e = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sErRgweMwXdy"
      },
      "outputs": [],
      "source": [
        "evalPolSoft = pad_policy(evalPolSoft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9S1TOIUs4Hq"
      },
      "source": [
        "### Varying Episode Length $T$ With $\\epsilon_{e} = 0.8$ (Policy Divergence $1.6^{T}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm_WbGSKs4H1"
      },
      "source": [
        "Set up evaluation policy, generate data and convert into factored format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZbt8rNXs4H1"
      },
      "outputs": [],
      "source": [
        "EVAL_EPSILON = 0.8\n",
        "\n",
        "evalPolSoft = np.copy(fullPol)\n",
        "evalPolSoft[evalPolSoft == 1] = 1 - EVAL_EPSILON\n",
        "evalPolSoft[evalPolSoft == 0] = EVAL_EPSILON / (n_actions - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnivT1wCs4H2",
        "outputId": "5470c21a-096b-4b68-8589-92789842ea69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.5999999999999996\n"
          ]
        }
      ],
      "source": [
        "# Calculate policy divergence from Voloshin et al.\n",
        "D = 0\n",
        "for state in range(randPol.shape[0] - 1):\n",
        "    for action in range(randPol.shape[1] - 1):\n",
        "        difference = evalPolSoft[state, action]/randPol[state, action]\n",
        "        D = max(D, difference)\n",
        "print(D)\n",
        "shorter_D = round(D,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGg2o2-Js4H2"
      },
      "outputs": [],
      "source": [
        "dgen = DataGenerator()\n",
        "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
        "    NSIMSAMPS, NSTEPS, policy=evalPolSoft, policy_idx_type='full',\n",
        "    p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
        "\n",
        "obs_samps = utils.format_dgen_samps(\n",
        "    states, actions, rewards, diab, NSTEPS, NSIMSAMPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWKUOE2Ls4H2",
        "outputId": "bf2854b3-5607-4a18-f6af-339f0fe4f10b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000, 20, 5)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time = np.arange(NSTEPS)\n",
        "times = np.stack(axis=0, arrays=[time]*NSIMSAMPS)\n",
        "times = times[..., np.newaxis]\n",
        "\n",
        "nf_tr_e = np.concatenate((times, states[:, 0:NSTEPS, :], actions, rewards, states[:, 1:, :]), axis=2)\n",
        "nf_tr_e.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4k3ggSqwy3X"
      },
      "outputs": [],
      "source": [
        "evalPolSoft = pad_policy(evalPolSoft)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7Zt7QLKc3iQ7",
        "5W3Fj_q5EADA",
        "TPF8wN8gNZEK"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}